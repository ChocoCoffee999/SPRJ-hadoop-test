version: "3.8"

services:
  dn2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.3-java8
    container_name: dn2
    hostname: dn2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://${MASTER_IP}:8020
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_datanode_hostname=192.168.0.3
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
    volumes:
      - ./hadoop/conf:/opt/hadoop/etc/hadoop
      - datanode:/hadoop/dfs/data
    ports:
      - "9866:9866"
      - "9867:9867"
      - "9864:9864"
    restart: unless-stopped

  spark-worker:
    image: bitnami/spark:3.5.6
    container_name: spark-worker
    hostname: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}          # spark://192.168.0.2:7077
      - SPARK_LOCAL_IP=${SPARK_LOCAL_IP}              # 이 장비의 IP (예: 192.168.0.3)
      - SPARK_PUBLIC_DNS=${SPARK_LOCAL_IP}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}      # 4
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}    # 18g
      - SPARK_WORKER_PORT=${SPARK_WORKER_PORT}        # 7078
      - SPARK_WORKER_WEBUI_PORT=${SPARK_WORKER_WEBUI} # 8081
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    volumes:
      - ./hadoop/conf:/opt/hadoop-conf
      - spark_worker_conf:/opt/bitnami/spark/conf           # spark-defaults.conf 여기
      - ./spark/conf/worker/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    ports:
      - "${SPARK_WORKER_WEBUI}:${SPARK_WORKER_WEBUI}"   # 8081
      - "${SPARK_SHUFFLE_PORT}:${SPARK_SHUFFLE_PORT}"   # 7337
      - "${SPARK_BLOCKMANAGER_PORT}:${SPARK_BLOCKMANAGER_PORT}" # 6060
      - "${SPARK_DRIVER_BM_PORT}:${SPARK_DRIVER_BM_PORT}"       # 6061
      - "${SPARK_EXECUTOR_RPC_PORT}:${SPARK_EXECUTOR_RPC_PORT}" # 6062
      - "${SPARK_WORKER_PORT}:${SPARK_WORKER_PORT}"             # 7078
    restart: unless-stopped

volumes:
  datanode:
  spark_worker_conf: